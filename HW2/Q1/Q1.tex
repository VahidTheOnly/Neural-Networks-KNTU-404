\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{xepersian}
\settextfont{Amiri}
\setlatintextfont{Times New Roman}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{geometry}
\geometry{margin=2.5cm}
\usepackage{graphicx}
\usepackage{caption}

\title{تکلیف دوم درس شبکه های عصبی}
\author{وحید ملکی، پوریا دادستان}
\date{\today}

\begin{document}
	
	\maketitle
	
	\section*{سوال ۱}
	\textbf{توضیح دهید چرا شبکه‌های عصبی اغلب به عنوان "جعبه سیاه" شناخته می‌شوند و این مسئله چه چالش‌هایی برای قابلیت تفسیر و شفافیت مدل ایجاد می‌کند و راه‌های رفع این مشکل چیست؟}
	
	\subsection*{۱. چرا شبکه‌های عصبی "جعبه سیاه" \lr{(Black Box)} هستند؟}
	اصطلاح "جعبه سیاه" در هوش مصنوعی به مدل‌هایی اشاره دارد که فرآیند تصمیم‌گیری درونی آن‌ها برای انسان به سادگی قابل درک نیست. دلایل اصلی این ویژگی در شبکه‌های عصبی عبارتند از:
	
	\begin{itemize}
		\item \textbf{پیچیدگی ساختاری و ابعاد بالا:} شبکه‌های عصبی عمیق \lr{(Deep Neural Networks)} اغلب شامل میلیون‌ها یا حتی میلیاردها پارامتر (وزن‌ها و بایاس‌ها) هستند. ردیابی اینکه چگونه یک ورودی خاص از طریق این اتصالات متعدد به یک خروجی منجر می‌شود، عملاً برای انسان غیرممکن است.
		\item \textbf{غیرخطی بودن:} استفاده از توابع فعال‌ساز غیرخطی (مانند ReLU، Sigmoid یا Tanh) در لایه‌های پنهان باعث می‌شود که رابطه بین ورودی و خروجی یک نگاشت ریاضی بسیار پیچیده باشد که نمی‌توان آن را با یک رابطه خطی ساده توضیح داد.
		\item \textbf{نمایش ویژگی‌های انتزاعی:} لایه‌های میانی شبکه، ویژگی‌هایی را استخراج می‌کنند که اغلب برای انسان معنای بصری یا منطقی مستقیمی ندارند (مثلاً ترکیبی از پیکسل‌ها که نه خط است و نه دایره، اما برای شبکه معنی‌دار است).
	\end{itemize}
	
	\subsection*{۲. چالش‌های ناشی از عدم تفسیرپذیری}
	عدم شفافیت در عملکرد شبکه‌های عصبی چالش‌های جدی‌ای را به وجود می‌آورد، از جمله:
	
	\begin{itemize}
		\item \textbf{عدم اعتماد \lr{(Lack of Trust)}:} در حوزه‌های حساس مانند پزشکی (تشخیص سرطان) یا مالی (تایید وام)، کاربران و متخصصان نیاز دارند بدانند چرا مدل یک تصمیم خاص را گرفته است. بدون توضیح منطقی، اعتماد به سیستم دشوار است.
		\item \textbf{عیب‌یابی دشوار \lr{(Debugging Difficulty)}:} وقتی یک شبکه عصبی اشتباه می‌کند، تشخیص اینکه کدام لایه یا کدام نورون باعث خطا شده است بسیار دشوار است، که فرآیند بهبود مدل را کند می‌کند.
		\item \textbf{سوگیری و تبعیض \lr{(Bias and Fairness)}:} مدل‌ها ممکن است سوگیری‌های موجود در داده‌های آموزشی را یاد بگیرند (مثلاً تبعیض نژادی یا جنسیتی). در یک جعبه سیاه، کشف این سوگیری‌ها قبل از وقوع فاجعه سخت است.
		\item \textbf{مسائل قانونی و مقرراتی:} قوانینی مانند GDPR در اروپا بر "حق توضیح" \lr{(Right to Explanation)} تاکید دارند. استفاده از مدل‌های جعبه سیاه در تصمیم‌گیری‌هایی که بر زندگی افراد تاثیر می‌گذارد، ممکن است با موانع قانونی روبرو شود.
	\end{itemize}
	
	\subsection*{۳. راه‌های رفع مشکل: هوش مصنوعی توضیح‌پذیر (XAI)}
	برای مقابله با این چالش‌ها، حوزه "هوش مصنوعی توضیح‌پذیر" (Explainable AI) راهکارهایی را ارائه می‌دهد:
	
	\begin{itemize}
		\item \textbf{روش‌های مستقل از مدل \lr{(Model-Agnostic Methods)}:} تکنیک‌هایی مانند \lr{LIME} و \lr{SHAP} سعی می‌کنند رفتار مدل پیچیده را با تقریب زدن آن به یک مدل ساده‌تر (مثل رگرسیون خطی) در همسایگی یک نمونه خاص توضیح دهند. این روش‌ها نشان می‌دهند کدام ویژگی‌های ورودی بیشترین تاثیر را بر تصمیم نهایی داشته‌اند.
		\item \textbf{بصری‌سازی (Visualization):} در شبکه‌های کانولوشنی (CNN)، می‌توان از روش‌هایی مانند \textit{Saliency Maps} یا \textit{Grad-CAM} استفاده کرد تا مشخص شود شبکه به کجای تصویر توجه کرده است (Heatmap).
		\item \textbf{مکانیزم توجه \lr{(Attention Mechanisms)}:} در مدل‌های پردازش زبان (مانند ترنسفورمرها)، مکانیزم توجه نشان می‌دهد که مدل هنگام تولید خروجی، روی کدام کلمات ورودی تمرکز کرده است که خود نوعی تفسیرپذیری ذاتی ایجاد می‌کند.
		\item \textbf{استفاده از مدل‌های تفسیرپذیر:} در مواردی که شفافیت اولویت بالاتری نسبت به دقت نهایی دارد، می‌توان به جای شبکه‌های عصبی عمیق از مدل‌های ساده‌تر مانند درخت تصمیم \lr{(Decision Trees)} یا رگرسیون لجستیک استفاده کرد.
	\end{itemize}
	
\end{document}