\documentclass[12pt,a4paper]{article}
\usepackage{amsmath,amssymb}
\usepackage{xepersian}
\settextfont{Amiri}
\setlatintextfont{Times New Roman}
\usepackage{geometry}
\geometry{margin=2.5cm}
\usepackage{booktabs}
\usepackage{url}
\usepackage{float}

\title{تکلیف اول درس شبکه‌های عصبی}
\author{وحید ملکی \\ شماره دانشجویی: 40313004}
\date{\today}

\begin{document}
	
	\maketitle
	
	\section*{سوال ۴}
	
	\subsection*{الف: محو شدن و انفجار گرادیان چیست؟}
	
	در آموزش شبکه‌های عصبی عمیق، گرادیان تابع هزینه نسبت به وزن‌های لایه‌های اولیه با استفاده از قاعده زنجیره‌ای و به صورت ضرب مکرر مشتق توابع فعال‌سازی و وزن‌های لایه‌های بعدی محاسبه می‌شود.
	
	\begin{itemize}
		\item \textbf{محو شدن گرادیان (\lr{Vanishing Gradient}):}  
		زمانی رخ می‌دهد که مقادیر مطلق مشتق توابع فعال‌سازی (مانند \lr{Sigmoid} و \lr{Tanh}) در نواحی اشباع کمتر از ۱ باشد. با افزایش تعداد لایه‌ها، ضرب مکرر اعداد کوچکتر از ۱ باعث می‌شود گرادیان در لایه‌های ابتدایی به صورت نمایی به صفر نزدیک شود.  
		\textbf{نتیجه عملی:} لایه‌های اولیه تقریباً هیچ به‌روزرسانی دریافت نمی‌کنند، ویژگی‌های سطح پایین (مانند لبه‌ها و گوشه‌ها در بینایی ماشین) به خوبی یاد گرفته نمی‌شوند و کل شبکه عملاً به یک شبکه کم‌عمق تبدیل می‌شود.
		
		\item \textbf{انفجار گرادیان (\lr{Exploding Gradient}):}  
		زمانی رخ می‌دهد که مقادیر مطلق مشتق‌ها یا وزن‌ها بزرگتر از ۱ باشند. ضرب مکرر این اعداد باعث رشد نمایی گرادیان می‌شود و مقدار آن به سرعت به اعداد بسیار بزرگ یا حتی \lr{NaN} تبدیل می‌شود.  
		\textbf{نتیجه عملی:} وزن‌ها تغییرات بسیار شدیدی می‌کنند، تابع هزینه نوسانات شدید پیدا می‌کند و آموزش کاملاً ناپایدار می‌شود.
	\end{itemize}
	
	\subsection*{ب: روش کلاسیک و بسیار موفق — نرمال‌سازی دسته‌ای (\lr{Batch Normalization})}
	
	\lr{Batch Normalization} که در سال ۲۰۱۵ توسط \lr{Sergey Ioffe} و \lr{Christian Szegedy} معرفی شد، تا چند سال استاندارد طلایی برای حل این دو مشکل بود.
	
	فرمول کامل \lr{BatchNorm} در زمان آموزش:
	\[
	\mu_B = \frac{1}{m}\sum_{i=1}^m x_i \quad ,\quad 
	\sigma_B^2 = \frac{1}{m}\sum_{i=1}^m (x_i - \mu_B)^2
	\]
	\[
	\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \quad ,\quad 
	y_i = \gamma \hat{x}_i + \beta
	\]
	
	که $\gamma$ و $\beta$ پارامترهای قابل یادگیری هستند و در زمان استنتاج از میانگین و واریانس متحرک استفاده می‌شود.
	
	\textbf{چرا \lr{BatchNorm} این مشکلات را حل می‌کند؟}
	\begin{itemize}
		\item توزیع ورودی هر لایه را به میانگین صفر و واریانس یک می‌رساند $\Rightarrow$ از ورود به نواحی اشباع توابع فعال‌سازی جلوگیری می‌کند.
		\item گرادیان‌ها را در مقیاس مناسب نگه می‌دارد $\Rightarrow$ هم از محو شدن و هم از انفجار جلوگیری می‌کند.
		\item اجازه استفاده از نرخ یادگیری بسیار بالاتر را می‌دهد.
		\item اثر منظم‌سازی دارد و نیاز به \lr{Dropout} را کاهش می‌دهد.
	\end{itemize}
	
	\textbf{معایب مهم \lr{BatchNorm}:}
	\begin{itemize}
		\item وابستگی شدید به سایز بچ (در \lr{Batch Size} کوچک، تخمین $\mu$ و $\sigma$ نویزدار می‌شود)
		\item افزایش مصرف حافظه (نیاز به ذخیره آمار بچ)
		\item مشکل در شبکه‌های بازگشتی و آموزش آنلاین
		\item تفاوت رفتار بین آموزش و استنتاج
	\end{itemize}
	
	\subsection*{ج: مقاله علمی پیشنهادی — حذف کامل نرمال‌سازی با روشی هوشمندانه}
	
	\textbf{عنوان مقاله:}  
	\lr{High-Performance Large-Scale Image Recognition Without Normalization}
	
	\textbf{نویسندگان:} \lr{Andrew Brock}, \lr{Soham De}, \lr{Samuel L. Smith} (همه از \lr{DeepMind})
	
	\textbf{کنفرانس/سال:} \lr{ICML 2021}
	
	\textbf{لینک کامل:} \url{https://arxiv.org/abs/2102.06171}
	
	این مقاله یکی از مهم‌ترین مقالات سال ۲۰۲۱ در حوزه پایداری آموزش شبکه‌های عمیق است و نشان می‌دهد که نه تنها می‌توان \lr{BatchNorm} را حذف کرد، بلکه با حذف آن می‌توان به عملکرد بهتری هم رسید!
	
	\subsection*{ایده اصلی مقاله: شبکه‌های بدون نرمال‌سازی (\lr{Normalizer-Free Nets} یا \lr{NFNet})}
	
	نویسندگان مشاهده کردند که \lr{BatchNorm} در واقع دو نقش دارد:
	1. پایدار کردن آموزش (جلوگیری از \lr{Vanishing/Exploding Gradient})
	2. منظم‌سازی
	
	آنها نشان دادند که نقش اول را می‌توان با روشی بسیار ساده‌تر و کارآمدتر جایگزین کرد: \textbf{برش تطبیقی گرادیان (\lr{Adaptive Gradient Clipping — AGC})}
	
	\subsection*{توضیح کامل روش \lr{Adaptive Gradient Clipping (AGC)}}
	
	روش کلاسیک \lr{Gradient Clipping} یک آستانه ثابت (مثلاً ۱.۰) روی نرم گرادیان کل شبکه اعمال می‌کند. اما \lr{AGC} هوشمندانه‌تر عمل می‌کند:
	
	برای هر پارامتر $w$ در هر لایه، نسبت نرم گرادیان به نرم وزن محاسبه می‌شود:
	\[
	r = \frac{\| \nabla_w \mathcal{L} \|_2}{\| w \|_2 + \epsilon}
	\]
	
	اگر این نسبت از یک آستانه $\lambda$ بیشتر باشد، گرادیان آن لایه به صورت زیر کوچک می‌شود:
	\[
	\nabla_w \mathcal{L} \leftarrow \nabla_w \mathcal{L} \times \min\left(1,\ \frac{\lambda}{r}\right)
	\]
	
	به عبارت ساده‌تر:  
	«تغییر وزن در هر قدم نباید بیشتر از $\lambda$ برابر مقدار فعلی وزن باشد.»
	
	این کار باعث می‌شود:
	- وزن‌ها هرگز تغییرات بسیار شدید نداشته باشند $\Rightarrow$ انفجار گرادیان غیرممکن می‌شود.
	- در عین حال گرادیان‌های مفید و معقول حفظ می‌شوند (برخلاف برش ثابت که ممکن است گرادیان‌های بزرگ اما مفید را هم قطع کند).
	
	\subsection*{نتایج شگفت‌انگیز مقاله}
	
	\begin{itemize}
		\item مدل \lr{NFNet-F6} بدون هیچ نوع نرمال‌سازی (نه \lr{BatchNorm}، نه \lr{LayerNorm}، نه \lr{GroupNorm}) به دقت \lr{88.5\% Top-1} در \lr{ImageNet} رسید که در زمان انتشار رکورد جدیدی بود.
		\item آموزش پایدار حتی با \lr{Batch Size = ۱۶} یا کمتر.
		\item سرعت آموزش تا ۸۰٪ بالاتر از مدل‌های معادل با \lr{BatchNorm}.
		\item مصرف حافظه به شدت کاهش یافت.
		\item شبکه‌های تا بیش از ۱۰۰۰ لایه بدون مشکل آموزش داده شدند.
	\end{itemize}
	
	\subsection*{مزایای روش \lr{AGC} و \lr{NFNet}}
	
	\begin{itemize}
		\item حذف کامل تمام لایه‌های نرمال‌سازی $\Rightarrow$ کد ساده‌تر، سرعت بالاتر، حافظه کمتر
		\item کاملاً مستقل از سایز بچ $\Rightarrow$ ایده‌آل برای آموزش توزیع‌شده و دستگاه‌های با حافظه محدود
		\item امکان استفاده از نرخ یادگیری بسیار بالا
		\item جلوگیری همزمان و مؤثر از هر دو مشکل \lr{Vanishing} و \lr{Exploding Gradient}
		\item دستیابی به دقت \lr{State-of-the-Art} بدون هیچ ترفند اضافی
		\item قابل تعمیم به معماری‌های دیگر (بعداً روی \lr{Transformer}ها و \lr{ViT}ها هم موفق بوده)
	\end{itemize}
	
	\subsection*{محدودیت‌ها و نکات عملی}
	
	\begin{itemize}
		\item نیاز به تنظیم دقیق $\lambda$ (معمولاً بین ۰٫۰۱ تا ۰٫۱ بسته به معماری)
		\item محاسبه نرم وزن در هر لایه هزینه محاسباتی جزئی اضافه می‌کند (ولی بسیار کمتر از \lr{BatchNorm})
		\item در مقاله اصلی فقط روی شبکه‌های کانولوشنی تست شده بود (هرچند مقالات بعدی نشان دادند روی \lr{Transformer}ها هم عالی کار می‌کند)
	\end{itemize}
	
	\subsection*{جمع‌بندی نهایی}
	
	مشکلات \lr{Vanishing} و \lr{Exploding Gradient} از مهم‌ترین موانع آموزش شبکه‌های عمیق بودند که با \lr{Batch Normalization} تا حد زیادی حل شدند. اما مقاله \lr{ICML 2021} از \lr{DeepMind} با معرفی \lr{Adaptive Gradient Clipping} نشان داد که می‌توان بدون هیچ نوع نرمال‌سازی و فقط با یک خط کد ساده، شبکه‌های عمیق‌تر، سریع‌تر و دقیق‌تری ساخت. این روش امروزه یکی از قدرتمندترین و مدرن‌ترین تکنیک‌های پایدارسازی آموزش شبکه‌های عصبی عمیق محسوب می‌شود و به سرعت در جامعه تحقیقاتی و صنعتی در حال گسترش است.
	
\end{document}