\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{xepersian}
\settextfont{Amiri}
\setlatintextfont{Times New Roman}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{geometry}
\geometry{margin=2.5cm}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{float}
\usepackage{array} 

\title{تکلیف اول درس شبکه های عصبی}
\author{وحید ملکی \\ شماره دانشجویی: 40313004}
\date{\today}

\begin{document}
	
	\maketitle
	
	\section*{سوال ۲}
	
	مجموعه داده مسکن کالیفرنیا، که از سرشماری سال ۱۹۹۰ ایالات متحده استخراج شده است، یکی از مجموعه داده‌های معروف برای مسائل رگرسیون چندمتغیره است. این داده‌ها شامل ویژگی‌های اقتصادی، جمعیتی و جغرافیایی مناطق مسکونی در کالیفرنیا بوده و هدف، پیش‌بینی \lr{Median House Value} است.
	
	\subsection*{الف: رسم و تفسیر توزیع (هیستوگرام) هر یک از ویژگی‌ها}
	
	با استفاده از کتابخانه \lr{sklearn}، داده‌ها بارگذاری شده و هیستوگرام تمام ویژگی‌ها و متغیر هدف با ۵۰ ستون (bin) رسم گردید. شکل \ref{fig:distributions} توزیع هر یک از ویژگی‌ها را نشان می‌دهد.
	
	\begin{figure}[h]
		\centering
		 \includegraphics[width=\textwidth]{california_housing_distributions.png}
		\caption{توزیع (هیستوگرام) ویژگی‌های مجموعه داده مسکن کالیفرنیا و متغیر هدف}
		\label{fig:distributions}
	\end{figure}
	
	\subsubsection*{تفسیر هیستوگرام‌ها}
	
	در ادامه، توزیع هر ویژگی به همراه تحلیل و اطلاعات استخراج‌شده از آن بررسی می‌شود:
	
	\begin{enumerate}
		\item \textbf{\lr{MedInc} (درآمد متوسط خانوار):}  
		توزیع چپ‌کشیده (\lr{right-skewed}) با پیک در حدود ۳ تا ۴ واحد (صد هزار دلار). بیشتر خانوارها درآمد متوسط پایینی دارند و تعداد کمی درآمد بسیار بالا (تا ۱۵) دارند. این نشان‌دهنده نابرابری درآمدی در مناطق است.
		
		\item \textbf{\lr{HouseAge} (سن متوسط خانه):}  
		توزیع نسبتاً یکنواخت با چند پیک در سنین ۱۵، ۳۰ و ۵۰ سال. این نشان می‌دهد خانه‌ها در سنین مختلف به طور نسبتاً یکسان توزیع شده‌اند، اما ساخت خانه در دهه‌های خاصی (مثلاً پس از جنگ جهانی دوم) بیشتر بوده است.
		
		\item \textbf{\lr{AveRooms} (میانگین تعداد اتاق در هر خانه):}  
		توزیع شدیداً چپ‌کشیده با پیک در حدود ۵ اتاق و دم بسیار طولانی تا بیش از ۱۲۰ اتاق. وجود مقادیر بسیار بالا (مثلاً خانه‌هایی با بیش از ۱۰۰ اتاق) نشان‌دهنده \textbf{داده‌های پرت (outlier)} یا احتمالاً خطا در داده‌هاست. این ویژگی نیاز به نرمال‌سازی یا حذف پرت دارد.
		
		\item \textbf{\lr{AveBedrms} (میانگین تعداد اتاق خواب):}  
		مشابه \lr{AveRooms}، توزیع چپ‌کشیده با پیک در حدود ۱ تا ۱.۵ و دم طولانی تا بیش از ۳۰. مقادیر بسیار بالا غیرواقعی به نظر می‌رسند و احتمالاً ناشی از خطا یا مناطق خاص (مثلاً خوابگاه‌ها) هستند.
		
		\item \textbf{\lr{Population} (جمعیت منطقه):}  
		توزیع شدیداً چپ‌کشیده با پیک در حدود ۱۰۰۰ نفر و دم تا بیش از ۳۵۰۰۰. بیشتر مناطق جمعیت کمی دارند و تعداد کمی منطقه پرجمعیت هستند. این ویژگی نیز دارای پرت‌های قابل توجه است.
		
		\item \textbf{\lr{AveOccup} (میانگین تعداد افراد در هر خانه):}  
		توزیع چپ‌کشیده با پیک در حدود ۲.۵ تا ۳ و دم بسیار طولانی تا بیش از ۱۲۰۰. مقادیر بسیار بالا کاملاً غیرعادی هستند و نشان‌دهنده داده‌های پرت یا خطا در ثبت اطلاعات است.
		
		\item \textbf{\lr{Latitude} (عرض جغرافیایی):}  
		توزیع چندمداله با پیک‌های اصلی در حدود ۳۴، ۳۷ و ۳۹ درجه. این الگو با توزیع جغرافیایی مناطق مسکونی در کالیفرنیا (از جنوب تا شمال) همخوانی دارد.
		
		\item \textbf{\lr{Longitude} (طول جغرافیایی):}  
		توزیع چندمداله با پیک‌های اصلی در حدود −۱۲۲ و −۱۱۸ درجه. این نشان‌دهنده تمرکز جمعیت در نواحی ساحلی (لس آنجلس، سان فرانسیسکو) و دره مرکزی است.
		
		\item \textbf{\lr{Median House Value} (متغیر هدف):}  
		توزیع چپ‌کشیده با پیک در حدود ۱.۸ تا ۲ واحد (۱۸۰ تا ۲۰۰ هزار دلار) و یک \textbf{پیک غیرعادی در ۵ واحد (۵۰۰ هزار دلار)}. این پیک در انتهای بالایی به دلیل \textbf{سقف‌گذاری (capping)} در داده‌های اصلی است — در سرشماری ۱۹۹۰، قیمت‌های بالای ۵۰۰ هزار دلار به این مقدار گرد شده‌اند. این امر مدل‌سازی را پیچیده می‌کند و ممکن است نیاز به حذف یا اصلاح این نمونه‌ها باشد.
	\end{enumerate}
	
	\subsubsection*{جمع‌بندی اطلاعات استخراج‌شده از هیستوگرام‌ها}
	
	\begin{itemize}
		\item \textbf{توزیع‌های نامتقارن و چپ‌کشیده:} اکثر ویژگی‌ها (به‌ویژه \lr{MedInc}، \lr{AveRooms}، \lr{AveBedrms}، \lr{Population}، \lr{AveOccup}) توزیع چپ‌کشیده دارند و نیاز به \textbf{تبدیل لگاریتمی یا نرمال‌سازی} دارند.
		
		\item \textbf{وجود پرت‌های شدید:} ویژگی‌هایی مانند \lr{AveRooms}، \lr{AveBedrms} و \lr{AveOccup} شامل مقادیر بسیار غیرعادی هستند که باید با روش‌های \textbf{حذف پرت} یا \textbf{جایگزینی} مدیریت شوند.
		
		\item \textbf{سقف‌گذاری در متغیر هدف:} پیک در ۵۰۰ هزار دلار نشان‌دهنده \lr{censoring} است و مدل‌های رگرسیون خطی ممکن است در این ناحیه خطای سیستماتیک داشته باشند.
		
		\item \textbf{اطلاعات جغرافیایی ارزشمند:} توزیع \lr{Latitude} و \lr{Longitude} نشان‌دهنده الگوهای فضایی مشخص است که می‌تواند در مهندسی ویژگی (مثلاً محاسبه فاصله از ساحل) مفید باشد.
	\end{itemize}
	
	این تحلیل اولیه نشان می‌دهد که پیش‌پردازش داده (نرمال‌سازی، حذف پرت، تبدیل متغیرها) برای بهبود عملکرد مدل‌های یادگیری ماشین ضروری است.
	
	\subsection*{ب: مقایسه روش‌های نرمال‌سازی داده‌ها}
	
	در این بخش، چهار روش نرمال‌سازی زیر بر روی ویژگی‌های مجموعه داده مسکن کالیفرنیا اعمال شده و تأثیر هر یک بر توزیع داده‌ها، مقادیر پرت و آماره‌های توصیفی بررسی شده است:
	
	\begin{enumerate}
		\item \textbf{نرمال‌سازی مین‌مکس به بازه $[0, 1]$}
		\item \textbf{نرمال‌سازی مین‌مکس به بازه $[-1, 1]$}
		\item \textbf{نرمال‌سازی استاندارد (Z-score)}
		\item \textbf{نرمال‌سازی مقاوم (Robust Scaling)}
	\end{enumerate}
	
	شکل \ref{fig:normalization_hist} توزیع هیستوگرام هر ویژگی را قبل و بعد از اعمال هر روش نشان می‌دهد. همچنین شکل \ref{fig:boxplots} با استفاده از نمودار جعبه‌ای (boxplot)، تأثیر هر روش بر مدیریت داده‌های پرت را مقایسه می‌کند.
	
	\begin{figure}[H]
		\centering
		 \includegraphics[width=\textwidth]{normalization_comparison.png}
		\caption{مقایسه توزیع هیستوگرام ویژگی‌ها در حالت اصلی و پس از اعمال چهار روش نرمال‌سازی}
		\label{fig:normalization_hist}
	\end{figure}
	
	\begin{figure}[H]
		\centering
		 \includegraphics[width=0.9\textwidth]{boxplot_comparison.png}
		\caption{نمودار جعبه‌ای (boxplot) برای مقایسه تأثیر روش‌های نرمال‌سازی بر داده‌های پرت}
		\label{fig:boxplots}
	\end{figure}
	
	\subsubsection*{فرمول ریاضی و تفسیر هر روش}
	
	در ادامه، برای هر روش، \textbf{فرمول ریاضی} و \textbf{تفسیر عملی} آن (اینکه دقیقاً چه کاری انجام می‌دهد و چه تأثیری بر داده‌ها دارد) ارائه شده است:
	
	\begin{enumerate}
		\item \textbf{نرمال‌سازی مین‌مکس به بازه $[0, 1]$} \\
		\textbf{فرمول:} 
		\[
		x' = \frac{x - x_{\min}}{x_{\max} - x_{\min}}
		\]
		\textbf{تفسیر:} این روش کوچک‌ترین مقدار ویژگی را به ۰ و بزرگ‌ترین مقدار را به ۱ نگاشت می‌کند. تمام مقادیر دیگر به صورت خطی در این بازه توزیع می‌شوند. به عبارت دیگر، \textbf{مقیاس ویژگی را به بازه استاندارد [۰,۱] تبدیل می‌کند} بدون تغییر در شکل نسبی توزیع. این روش برای الگوریتم‌هایی که به مقیاس مطلق حساس هستند (مثل شبکه‌های عصبی با تابع فعال‌سازی سیگموید) مفید است، اما \textbf{بسیار حساس به پرت} است — یک مقدار پرت می‌تواند کل داده‌ها را فشرده کند.
		
		\item \textbf{نرمال‌سازی مین‌مکس به بازه $[-1, 1]$} \\
		\textbf{فرمول:} 
		\[
		x' = 2 \times \frac{x - x_{\min}}{x_{\max} - x_{\min}} - 1
		\]
		\textbf{تفسیر:} مشابه روش قبلی، اما بازه خروجی $[-1, 1]$ است. ابتدا داده‌ها به $[0,1]$ نگاشت می‌شوند، سپس با ضرب در ۲ و کم کردن ۱، به بازه متقارن حول صفر تبدیل می‌شوند. این روش برای الگوریتم‌هایی که به \textbf{مقیاس متقارن حول صفر} نیاز دارند (مثل برخی روش‌های بهینه‌سازی یا شبکه‌های عصبی با تابع فعال‌سازی tanh) مناسب است. حساسیت به پرت همچنان بالاست.
		
		\item \textbf{نرمال‌سازی استاندارد (Standard Scaling یا Z-score)} \\
		\textbf{فرمول:} 
		\[
		x' = \frac{x - \mu}{\sigma}
		\]
		\textbf{تفسیر:} این روش میانگین $(\mu)$ ویژگی را به ۰ و انحراف معیار $(\sigma)$ را به ۱ تبدیل می‌کند. به عبارت دیگر، \textbf{داده‌ها را به توزیع نرمال استاندارد (میانگین ۰، واریانس ۱) نزدیک می‌کند}. این روش فرض می‌کند داده‌ها تقریباً نرمال هستند و برای الگوریتم‌های مبتنی بر فاصله (مثل SVM، k-NN، رگرسیون خطی) بسیار مفید است. اما چون از میانگین و انحراف معیار استفاده می‌کند، \textbf{تأثیر پرت‌ها بسیار زیاد است} و می‌تواند توزیع را به شدت تحریف کند.
		
		\item \textbf{نرمال‌سازی مقاوم (Robust Scaling)} \\
		\textbf{فرمول:} 
		\[
		x' = \frac{x - \text{median}}{Q_3 - Q_1} \quad (\text{IQR} = Q_3 - Q_1)
		\]
		\textbf{تفسیر:} این روش از \textbf{میانه (median)} به جای میانگین و \textbf{دامنه بین‌چارکی (IQR)} به جای انحراف معیار استفاده می‌کند. میانه نماینده مرکزی مقاوم به پرت است و IQR فقط به ۵۰\% داده‌های مرکزی وابسته است. بنابراین، \textbf{این روش در حضور پرت‌ها بسیار مقاوم است} و توزیع نسبی داده‌های اصلی را بدون تحریف حفظ می‌کند. برای داده‌های واقعی با نویز و پرت (مثل این دیتاست) بهترین انتخاب است.
	\end{enumerate}
	
	\subsubsection*{تحلیل آماری کلی (از خروجی کد)}
	
	\begin{table}[H]
		\centering
		\caption{مقایسه آماری روش‌های نرمال‌سازی (محدوده میانگین، انحراف معیار، حداقل و حداکثر)}
		\label{tab:stats_comparison}
		\begin{tabular}{lcccc}
			\toprule
			\textbf{روش} & \textbf{محدوده میانگین} & \textbf{محدوده انحراف معیار} & \textbf{حداقل} & \textbf{حداکثر} \\
			\midrule
			اصلی & $[-119.57, 1425.48]$ & $[0.47, 1132.46]$ & $-124.35$ & $35682.00$ \\
			MinMax[0,1] & $[0.0019, 0.5420]$ & $[0.0084, 0.2468]$ & $0.0000$ & $1.0000$ \\
			MinMax[-1,1] & $[-0.9962, 0.0839]$ & $[0.0167, 0.4936]$ & $-1.0000$ & $1.0000$ \\
			Standard & $[\sim 10^{-15}, \sim 10^{-17}]$ & $[1.0000, 1.0000]$ & $-2.39$ & $119.42$ \\
			Robust & $[-0.2849, 0.5125]$ & $[0.5286, 12.1828]$ & $-7.66$ & $1455.12$ \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	\subsubsection*{مشاهدات کلیدی از هیستوگرام‌ها و باکس‌پلات‌ها}
	
	\begin{itemize}
		\item \textbf{MinMax [0,1] و [-1,1]:} این روش‌ها تمام مقادیر را به بازه محدود مین‌مکس می‌برند، اما \textbf{حساس به پرت} هستند. در ویژگی‌هایی مانند \lr{Population} و \lr{AveRooms}، پرت‌های شدید باعث فشرده شدن کل داده‌ها در نزدیکی ۰ یا −۱ می‌شوند.
		
		\item \textbf{Standard Scaling:} میانگین $\approx 0$ و انحراف معیار $= 1$ برای همه ویژگی‌ها. اما به دلیل استفاده از میانگین و انحراف معیار، \textbf{تأثیر پرت‌ها بسیار زیاد است} و توزیع پس از نرمال‌سازی همچنان چپ‌کشیده باقی می‌ماند.
		
		\item \textbf{Robust Scaling:} از میانه و دامنه بین‌چارکی (IQR) استفاده می‌کند که \textbf{مقاوم به پرت} هستند. در باکس‌پلات، مشاهده می‌شود که پرت‌ها در این روش \textbf{کمتر کشیده شده‌اند} و توزیع متعادل‌تری دارند.
	\end{itemize}
	
	---
	
	\subsection*{پ: کدام روش برای داده‌های دارای پرت بهتر است؟ چرا؟}
	
	\textbf{\lr{Robust Scaling} بهترین روش برای داده‌های دارای پرت است.}
	
	\subsubsection*{دلیل ریاضی}
	
	\begin{itemize}
		\item \textbf{میانه (Median):} برخلاف میانگین، تحت تأثیر مقادیر بسیار بزرگ یا کوچک قرار نمی‌گیرد. اگر ۱۰\% داده‌ها پرت باشند، میانه همچنان نماینده خوبی از مرکز داده‌هاست.
		
		\item \textbf{IQR = $Q_3 - Q_1$:} این معیار پراکندگی فقط به ۵۰\% داده‌های مرکزی وابسته است و پرت‌های دور (بالای $Q_3 + 1.5 \times IQR$ یا زیر $Q_1 - 1.5 \times IQR$) در محاسبه آن دخیل نیستند.
		
		\item \textbf{نتیجه:} نرمال‌سازی با میانه و IQR، \textbf{مقیاس داده‌های اصلی را بدون تحریف توسط پرت‌ها حفظ می‌کند}.
	\end{itemize}
	
	\subsubsection*{تأیید با نمونه از داده‌ها (ویژگی \lr{MedInc})}
	
	\begin{table}[H]
		\centering
		\caption{مقایسه ۱۰ نمونه اول ویژگی \lr{MedInc} پس از نرمال‌سازی}
		\label{tab:medinc_sample}
		\begin{tabular}{lrrrrr}
			\toprule
			نمونه & اصلی & MinMax[0,1] & MinMax[-1,1] & Standard & \textbf{Robust} \\
			\midrule
			۱ & 8.3252 & 0.5397 & 0.0793 & 2.3448 & \textbf{2.1976} \\
			۲ & 8.3014 & 0.5380 & 0.0761 & 2.3322 & \textbf{2.1867} \\
			۳ & 7.2574 & 0.4660 & -0.0679 & 1.7827 & \textbf{1.7077} \\
			$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
			۱۰ & 3.6912 & 0.2201 & -0.5598 & -0.0945 & \textbf{0.0717} \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	\begin{itemize}
		\item در \lr{MinMax}، مقادیر بالا (مثل ۸.۳) به نزدیکی ۰.۵ فشرده می‌شوند، در حالی که بیشتر داده‌ها در بازه ۰ تا ۰.۳ هستند → \textbf{از دست رفتن وضوح}.
		
		\item در \lr{Standard}، به دلیل وجود پرت‌های درآمد بالا، انحراف معیار بزرگ شده و مقادیر معمولی (مثل ۳.۶) به نزدیکی صفر یا منفی می‌روند → \textbf{تحریف توزیع}.
		
		\item در \textbf{\lr{Robust}}، مقادیر منطقی (درآمد ۳ تا ۸) به بازه منطقی $[-1, 3]$ نگاشت می‌شوند و \textbf{تفاوت نسبی بین نمونه‌ها حفظ می‌شود}.
	\end{itemize}
	
	\subsubsection*{شواهد بصری از باکس‌پلات (شکل \ref{fig:boxplots})}
	
	\begin{itemize}
		\item در روش‌های \lr{MinMax} و \lr{Standard}، پرت‌ها (نقاط سیاه) بسیار دور از جعبه هستند و \textbf{توزیع نامتعادل} است.
		\item در \lr{Robust}، جعبه‌ها متعادل‌تر و پرت‌ها \textbf{کمتر برجسته} هستند → نشان‌دهنده \textbf{مدیریت بهتر نویز}.
	\end{itemize}
	
	\subsubsection*{نتیجه‌گیری نهایی}
	
	\textbf{\lr{Robust Scaling} به دلیل استفاده از آماره‌های مقاوم (میانه و IQR)، بهترین عملکرد را در حضور داده‌های پرت دارد و توزیع نسبی داده‌ها را بدون تحریف حفظ می‌کند. این روش به‌ویژه برای مدل‌های حساس به مقیاس مانند شبکه‌های عصبی، SVM و k-NN توصیه می‌شود.}
	
\end{document}